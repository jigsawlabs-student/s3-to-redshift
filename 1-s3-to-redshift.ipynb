{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "emotional-rhythm",
   "metadata": {},
   "source": [
    "# From S3 to Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-leadership",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this lesson, we'll use redshift to store data that originally came from the `foursquare_api` application about our venues zipcodes.  We currently have the data in CSV form, and we'll load this CSV file from to S3, and then import the data over to redshift.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-check",
   "metadata": {},
   "source": [
    "### Connecting to Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-clearing",
   "metadata": {},
   "source": [
    "Let's get started by connecting to our redshift database.  Because redshift is derived from postgres, we can use our `psycopg2` library to connect to the database.  To do so, we'll need to provide our redshift endpoint as the host.  We can find this on the redshift dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-yesterday",
   "metadata": {},
   "source": [
    "> <img src=\"./redshift-cluster.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-times",
   "metadata": {},
   "source": [
    "We'll also need to specify the user, password, and database name that we specified when creating our redshift cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "enhanced-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "endpoint = \"redshift-cluster-1.cdpgnoufdsdf.us-east-1.redshift.amazonaws.com\"\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=endpoint,\n",
    "    database=\"dev\",\n",
    "    port = \"5439\",\n",
    "    user=\"awsuser\",\n",
    "    password=\"Password1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "sonic-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-scenario",
   "metadata": {},
   "source": [
    "Now that we've connected to the database, the next step is to create a table in redshift and load in some data from a csv file stored in S3.  \n",
    "\n",
    "So let's create the table our zipcodes table in redshift.  Below is the command to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "reasonable-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_zipcodes_query = \"\"\"CREATE TABLE \"zipcodes\" (\n",
    "    \"id\" integer NOT NULL DEFAULT nextval('zipcodes_id_seq'),\n",
    "    \"code\" INTEGER,\n",
    "    \"city_id\" INTEGER\n",
    ");\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "behavioral-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(create_zipcodes_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cathedral-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-database",
   "metadata": {},
   "source": [
    "### Loading Data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-variation",
   "metadata": {},
   "source": [
    "For now, we can load in data to S3 simply by uploading it through the AWS console.  We can visit the S3 dashboard [here](https://s3.console.aws.amazon.com/s3).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-privacy",
   "metadata": {},
   "source": [
    "> <img src=\"./s3-dashboard.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-domestic",
   "metadata": {},
   "source": [
    "We can add in some data by creating a new bucket.  And then simply dragging and dropping a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-temperature",
   "metadata": {},
   "source": [
    "<img src=\"./s3-data.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-raising",
   "metadata": {},
   "source": [
    "### Adding Some Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-height",
   "metadata": {},
   "source": [
    "Now that our data is in S3, we can copy it from S3 into AWS redshift.  Here's how we do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "comprehensive-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"COPY zipcodes (id, code, city_id) from 's3://jigsaw-sample-data/zipcodes.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::095598444804:role/redshiftRole'\n",
    "delimiter ','\n",
    "IGNOREHEADER 1\n",
    "region 'us-east-1';\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "smoking-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "massive-quilt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(47, 19019, 52), (48, 10001, 50)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT * FROM zipcodes LIMIT 2;')\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-joshua",
   "metadata": {},
   "source": [
    "Ok, so let's break down thee command above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-portsmouth",
   "metadata": {},
   "source": [
    "```sql\n",
    "COPY zipcodes (id, code, city_id) from 's3://jigsaw-sample-data/zipcodes.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-technique",
   "metadata": {},
   "source": [
    "The command takes the form `COPY table_name (table columns) from s3://csv_file.csv`.  Then in the next line we specify the credentials that grants us access to the S3 file.  Notice that we have a key of the `aws_iam_role` and that the role we are using is the `redshiftRole`.  We get the Role ARN by going to that role's dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-imaging",
   "metadata": {},
   "source": [
    "> <img src=\"./redshift-role.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-theta",
   "metadata": {},
   "source": [
    "Then finally, we provided optional parameters of the delimeter and IGNOREHEADER 1, and finished with the redshift region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-inventory",
   "metadata": {},
   "source": [
    "> Take another look at the query and see if it makes more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"COPY zipcodes (id, code, city_id) from 's3://jigsaw-sample-data/zipcodes.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::095598444804:role/redshiftRole'\n",
    "delimiter ','\n",
    "IGNOREHEADER 1\n",
    "region 'us-east-1';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-blast",
   "metadata": {},
   "source": [
    "### Debugging Copy Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-token",
   "metadata": {},
   "source": [
    "Unfortunately, redshift is fairly strict in terms of it's copy commands.  To debug redshift, there are a couple of things that we can do.\n",
    "\n",
    "The first, is to attempt the query directly through the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-lebanon",
   "metadata": {},
   "source": [
    "> <img src=\"./query_editor.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-queensland",
   "metadata": {},
   "source": [
    "The next is to take a look at what went wrong by reading the error message.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-stephen",
   "metadata": {},
   "source": [
    "Let's see this by way of example.  We'll provide an invalid query by removing the `IGNOREHEADER 1` flag.  This time around, redshift will try to read in the first line of the csv as values, and will see that the first row of `id, code, city_id` cannot be inserted into the zipcode table as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "occupational-philosophy",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError_",
     "evalue": "Load into table 'zipcodes' failed.  Check 'stl_load_errors' system table for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError_\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-d46aa21d6107>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mInternalError_\u001b[0m: Load into table 'zipcodes' failed.  Check 'stl_load_errors' system table for details.\n"
     ]
    }
   ],
   "source": [
    "qry = \"\"\"COPY zipcodes (id, code, city_id) from 's3://jigsaw-sample-data/zipcodes.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::095598444804:role/redshiftRole'\n",
    "delimiter ','\n",
    "region 'us-east-1';\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-sewing",
   "metadata": {},
   "source": [
    "Unfortunately, the error message is not directly apparent.  We see an error message of the following:\n",
    "```python\n",
    "Load into table 'zipcodes' failed.  Check 'stl_load_errors' system table for details.\n",
    "```\n",
    "\n",
    "To see what went wrong we'll have to query the `stl_load_errors` table to view the most recent errors.  Below is the query to view the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "judicial-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"select query, substring(filename,22,25) as filename,line_number as line, \n",
    "substring(colname,0,12) as column, type, position as pos, substring(raw_line,0,30) as line_text,\n",
    "substring(raw_field_value,0,15) as field_text, \n",
    "substring(err_reason,0,45) as reason\n",
    "from stl_load_errors \n",
    "order by query desc\n",
    "limit 1;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fiscal-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "worldwide-passport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1041,\n",
       "  'ta/zipcodes.csv',\n",
       "  1,\n",
       "  'id',\n",
       "  'int4      ',\n",
       "  0,\n",
       "  'id,code,city_id',\n",
       "  'id',\n",
       "  \"Invalid digit, Value 'i', Pos 0, Type: Integ\")]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()\n",
    "\n",
    "# [(1041,\n",
    "#   'ta/zipcodes.csv',\n",
    "#   1,\n",
    "#   'id',\n",
    "#   'int4      ',\n",
    "#   0,\n",
    "#   'id,code,city_id',\n",
    "#   'id',\n",
    "#   \"Invalid digit, Value 'i', Pos 0, Type: Integ\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-session",
   "metadata": {},
   "source": [
    "And doing so, we can see the query number, the file that was read, the line number of the error (line 1), and in the last column, the reason.  Here, we had an our CSV file contained a value of `i` (from the id) when it needed an Integer.  So we can tell redshift to skip the header line, so that the values it reads in are the correct type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-liquid",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-xerox",
   "metadata": {},
   "source": [
    "In this lesson, we saw how to read data into our redshift database from S3.  We did so by first connecting to our database using `psycopg2` with a connecting to the redshift endpoint.  We then created our table in redshift.  Next was to copy over our data from S3 and into redshift, which we did with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"COPY zipcodes (id, code, city_id) from 's3://jigsaw-sample-data/zipcodes.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::095598444804:role/redshiftRole'\n",
    "delimiter ','\n",
    "IGNOREHEADER 1\n",
    "region 'us-east-1';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-transport",
   "metadata": {},
   "source": [
    "Finally, we saw that to debug our copy command in redshift we needed to `stl_load_errors` table to view a more detailed error message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-literature",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Loading Data From s3](https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data.html)\n",
    "\n",
    "[S3 to Redshift](https://www.sqlshack.com/load-data-into-aws-redshift-from-aws-s3/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
